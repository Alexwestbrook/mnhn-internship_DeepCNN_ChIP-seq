{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for data inspection\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from more_itertools import grouper\n",
    "from itertools import islice, chain, zip_longest, repeat\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.axes as ax\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from scipy import io\n",
    "from scipy.signal import convolve\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.cluster.hierarchy import cut_tree\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from Modules import utils, plot_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "data_dir = '../shared_folder'\n",
    "writing_dir = '../shared_folder'\n",
    "# writing_dir='../'\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bombyx long reads vs short reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{data_dir}/Bombyx/raw_data/X_long.npz') as f:\n",
    "    long_reads = f['reads']\n",
    "with np.load(f'{data_dir}/Bombyx/raw_data/X_reads.npz') as f:\n",
    "    reads = f['reads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for i in range(len(long_reads)):\n",
    "    assert(len(long_reads[i]) == len(long_reads[i].rstrip()))\n",
    "    total += len(long_reads[i])\n",
    "print(total)\n",
    "total2 = 0\n",
    "for i in range(len(reads)):\n",
    "    assert(len(reads[i]) == len(reads[i].rstrip()))\n",
    "    total2 += len(reads[i])\n",
    "print(total2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of reads vs number of reads without Ns in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40956444,)\n"
     ]
    }
   ],
   "source": [
    "with np.load(f'{data_dir}/Judith-H3K9me3/raw_data/Control_reads.npz') as f:\n",
    "    reads = f['reads']\n",
    "print(reads.shape)\n",
    "# sequences = utils.remove_reads_with_N(reads, tolerance=0)\n",
    "# print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of reads vs number of reads without Ns in post-selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'CENPT'\n",
    "model_name = 'model_inception2'\n",
    "threshold = 0.75\n",
    "with np.load(f'{data_dir}/{data}/results/{model_name}/seqs_{data}_over_{threshold}_with_{model_name}.npz') as f:\n",
    "    reads = f['reads']\n",
    "print(reads.shape)\n",
    "sequences = utils.remove_reads_with_N(reads, tolerance=0)\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read length check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = utils.check_read_lengths(reads)\n",
    "dico = dict(sorted(dico.items(), reverse=True))\n",
    "print('{read_length: nb_reads}')\n",
    "print(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate checks, in total vs without Ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{data_dir}/H3K9me3/dataset30M.npz') as f:\n",
    "    reads = f['x_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for read duplicates\n",
      "1 batches\n",
      "Processing batch 0\n",
      "seq 20000000/40956444 duplicates\n",
      "seq 40000000/40956444 duplicates\n",
      "seq 40956444/40956444 duplicates\n",
      "Looking for duplicate level duplicates\n",
      "1 batches\n",
      "Processing batch 0\n",
      "seq 20000000/39066493 duplicates\n",
      "seq 39066493/39066493 duplicates\n",
      "{duplicate_level: nb_unique_reads}\n",
      "{126: 1, 125: 1, 121: 1, 112: 1, 110: 1, 109: 1, 108: 1, 104: 2, 99: 1, 98: 2, 97: 2, 96: 1, 95: 1, 94: 2, 93: 1, 92: 1, 91: 1, 90: 1, 89: 1, 87: 1, 85: 2, 84: 1, 82: 2, 81: 4, 80: 2, 79: 3, 78: 1, 77: 2, 76: 2, 75: 2, 74: 6, 73: 8, 72: 4, 71: 7, 70: 7, 69: 5, 68: 5, 67: 11, 66: 10, 65: 11, 64: 13, 63: 16, 62: 10, 61: 16, 60: 18, 59: 22, 58: 17, 57: 21, 56: 32, 55: 19, 54: 35, 53: 43, 52: 33, 51: 32, 50: 52, 49: 46, 48: 35, 47: 46, 46: 53, 45: 53, 44: 61, 43: 86, 42: 87, 41: 89, 40: 88, 39: 105, 38: 101, 37: 122, 36: 142, 35: 152, 34: 173, 33: 171, 32: 203, 31: 197, 30: 230, 29: 261, 28: 315, 27: 326, 26: 388, 25: 389, 24: 457, 23: 529, 22: 573, 21: 705, 20: 804, 19: 958, 18: 1006, 17: 1247, 16: 1446, 15: 1784, 14: 2115, 13: 2512, 12: 2979, 11: 3653, 10: 4605, 9: 5495, 8: 6581, 7: 8384, 6: 10642, 5: 14235, 4: 21596, 3: 59651, 2: 969704, 1: 37940415}\n",
      "top 5 duplicate level reads: [126, 125, 121, 112, 110]\n"
     ]
    }
   ],
   "source": [
    "# with np.load(f'{data_dir}/HEK293-ZFAT/raw_data/Control_reads_deduped.npz') as f:\n",
    "#     reads = f['reads']\n",
    "# print(reads.shape)\n",
    "# sequences = utils.remove_reads_with_N(reads, tolerance=0)\n",
    "# print(len(sequences))\n",
    "\n",
    "print('Looking for read duplicates')\n",
    "dico = utils.find_duplicates(reads, one_hot=False, batch_size=50_000_000, print_freq=20_000_000)\n",
    "values = list(dico.values())\n",
    "print('Looking for duplicate level duplicates')\n",
    "dico2 = utils.find_duplicates(values, batch_size=100_000_000, print_freq=20_000_000)\n",
    "dico2 = dict(sorted(dico2.items(), reverse=True))\n",
    "print('{duplicate_level: nb_unique_reads}')\n",
    "print(dico2)\n",
    "print('top 5 duplicate level reads:', list(dico2.keys())[:5])\n",
    "\n",
    "# print('Looking for read duplicates')\n",
    "# dico = utils.find_duplicates(sequences, one_hot=False, batch_size=50_000_000, print_freq=20_000_000)\n",
    "# values = list(dico.values())\n",
    "# print('Looking for duplicate level duplicates')\n",
    "# dico2 = utils.find_duplicates(values, batch_size=100_000_000, print_freq=20_000_000)\n",
    "# dico2 = dict(sorted(dico2.items(), reverse=True))\n",
    "# print('{duplicate_level: nb_unique_reads}')\n",
    "# print(dico2)\n",
    "# print('top 5 duplicate level reads:', list(dico2.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAGACTTTACAAACAGAGTGTTTCCTAACTGCTCTATGAAAAGAAAGGTTAAACTCTGTGAGTTGAACGCACACATCACAAAGGAGTTTCTGAGAATCATT']\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "selected_reads = [k for k, v in dico.items() if v == 110]\n",
    "print(selected_reads)\n",
    "print(dico[selected_reads[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human telomere sequences check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human telomere sequences\n",
    "seq_list = [\n",
    "    'CTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC',\n",
    "    'CCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAA',\n",
    "    'CCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTA',\n",
    "    'ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCT',\n",
    "    'TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACC',\n",
    "    'AACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCC',\n",
    "    'GGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTA',\n",
    "    'GTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAG',\n",
    "    'TTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGG',\n",
    "    'TAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGG',\n",
    "    'AGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGT',\n",
    "    'GGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTT'\n",
    "]\n",
    "for seq in seq_list:\n",
    "    if seq in dico.keys():\n",
    "        print(dico[seq], seq)\n",
    "    else:\n",
    "        print(0, seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot duplicate levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(dico.keys())\n",
    "y = list(dico.values())\n",
    "plt.plot(x, y)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('occurences')\n",
    "plt.xlabel('duplicate level')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect simple dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of reads: 60000000\n",
      "train: 42000000 valid: 9000000 test: 9000000\n",
      "pos_train: 21000000 pos_valid 4500000 pos_test 4500000\n"
     ]
    }
   ],
   "source": [
    "with np.load(f'{data_dir}/Judith-H3K9me3/dataset.npz') as dataset:\n",
    "    y_train = dataset['y_train']\n",
    "    y_valid = dataset['y_valid']\n",
    "    y_test = dataset['y_test']\n",
    "print('total number of reads:',len(y_train) + len(y_valid) + len(y_test))\n",
    "print('train:', len(y_train), 'valid:', len(y_valid), 'test:', len(y_test))\n",
    "print('pos_train:', len(y_train[y_train == 1]), 'pos_valid', len(y_valid[y_valid == 1]), 'pos_test', len(y_test[y_test == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect sharded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{data_dir}/H3K9me3-GSE175752/sharded_dataset/train_12.npz') as f:\n",
    "    ids = f['ids']\n",
    "    one_hots = f['one_hots']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14803404,)\n",
      "(14803404, 76, 4)\n",
      "@SRR14678338.22955188 22955188 length=76 ATAAATATGTATTGAGCTTTTTCTATATGCCAGCAACTAGTGGAAATATAGCATTCAACAAGACAAACATTGTCCT\n",
      "@SRR14678332.23065510 23065510 length=76 TGCTTGACTTCTTGTCTCTAGTATTTGAAATCTTCCTTGCATATGATTGTCTCATTACCTTCCTAAAATCTAGTTC\n",
      "@SRR14678339.536513 536513 length=76 AGTATGAAAGAATAGAAGTGTTTCTCATAGCATCATGTCATCCTTCTGGAACCTAAGCACGTTCTAGTGAGAATGG\n",
      "@SRR14678336.26889983 26889983 length=76 CTCTTTTGACATCAAACCCTTTCCCCTTTTTTCTGCGCCATGTGTGAAGGACAGGCAAATGGTCTGAGAAGAGAAC\n",
      "@SRR14678339.17193672 17193672 length=76 GTGGTGGTGCGTGCTTCTAATCCCAGCTACTCAGGAGGCTGAGGCAGAAGAACCGCTTGAACCGGGAGGCAGAGGT\n",
      "@SRR14678330.13389704 13389704 length=76 GTTGTCATTTCCAATTAAGCAAGGGTTGTAATACAGGGAAGTGTGTTTCTAAAATTGTGAAATTGTTCTTATCTAT\n",
      "@SRR14678339.20987926 20987926 length=76 AAGAATGCCTTTAAGCAATTTTCTGCCCTGGGTGGGCCAGGTGTTCCTTGCCCTCATTCTGGTAAACCCACAACCT\n",
      "@SRR14678330.21345252 21345252 length=76 GTAGTTTGGATATTTGTCCTGCAAATCGCAAGTTGAAATTTGATCCCCACTTTTGGACTTAGAGCCTATTGAGAGG\n",
      "@SRR14678342.6755082 6755082 length=76 CCTTGAACTAAGCATAGTTTTTTCATCTTTAAAAAAAAAAAAAAAAAGAATAACATGGGCCCATAAAGCCCAAAAT\n",
      "@SRR14678337.27059000 27059000 length=76 GATACACTTCTGTCTTGTTCTCTTTTTTTTTTTTTTTTTTTTTTAAAACGGAGGAATGGAGTGTCCCCCAGGATGG\n",
      "@SRR14678345.33899978 33899978 length=76 CATAATAACACTCTCAAAACTTGAATTTGAAGGAAATTACCTTAAGTGGAAAAAAAGCATCTATCAAAAACCTACT\n"
     ]
    }
   ],
   "source": [
    "print(ids.shape)\n",
    "print(one_hots.shape)\n",
    "counter = 0\n",
    "for i, val in zip(ids, one_hots):\n",
    "    if counter > 10:\n",
    "        break\n",
    "    print(i, utils.one_hot_decode(val))\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (248956422, 4)\n",
      "2 (242193529, 4)\n",
      "3 (198295559, 4)\n",
      "4 (190214555, 4)\n",
      "5 (181538259, 4)\n",
      "6 (170805979, 4)\n",
      "7 (159345973, 4)\n",
      "8 (145138636, 4)\n",
      "9 (138394717, 4)\n",
      "10 (133797422, 4)\n",
      "11 (135086622, 4)\n",
      "12 (133275309, 4)\n",
      "13 (114364328, 4)\n",
      "14 (107043718, 4)\n",
      "15 (101991189, 4)\n",
      "16 (90338345, 4)\n",
      "17 (83257441, 4)\n",
      "18 (80373285, 4)\n",
      "19 (58617616, 4)\n",
      "20 (64444167, 4)\n",
      "21 (46709983, 4)\n",
      "22 (50818468, 4)\n",
      "X (156040895, 4)\n",
      "Y (57227415, 4)\n"
     ]
    }
   ],
   "source": [
    "hg38_chr_names = {\n",
    "    1: 'NC_000001.11',\n",
    "    2: 'NC_000002.12',\n",
    "    3: 'NC_000003.12',\n",
    "    4: 'NC_000004.12',\n",
    "    5: 'NC_000005.10',\n",
    "    6: 'NC_000006.12',\n",
    "    7: 'NC_000007.14',\n",
    "    8: 'NC_000008.11',\n",
    "    9: 'NC_000009.12',\n",
    "    10: 'NC_000010.11',\n",
    "    11: 'NC_000011.10',\n",
    "    12: 'NC_000012.12',\n",
    "    13: 'NC_000013.11',\n",
    "    14: 'NC_000014.9',\n",
    "    15: 'NC_000015.10',\n",
    "    16: 'NC_000016.10',\n",
    "    17: 'NC_000017.11',\n",
    "    18: 'NC_000018.10',\n",
    "    19: 'NC_000019.10',\n",
    "    20: 'NC_000020.11',\n",
    "    21: 'NC_000021.9',\n",
    "    22: 'NC_000022.11',\n",
    "    'X': 'NC_000023.11',\n",
    "    'Y': 'NC_000024.10'}\n",
    "for chr_id in hg38_chr_names.keys():\n",
    "    # load one_hot_encoded sequence\n",
    "    genome_file = f'{data_dir}/Human/assembly/GRCh38/chr{chr_id}.npz'\n",
    "    with np.load(genome_file) as f:\n",
    "        one_hot = f['one_hot_genome']\n",
    "        print(chr_id, one_hot.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf2.5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00e7424330818c90ac25a9eb217b83ab19236ec3c884b7e4b78b97ecfb399601"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
