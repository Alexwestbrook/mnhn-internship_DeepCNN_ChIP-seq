{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.axes as ax \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import io\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "import pandas as pd\n",
    "import Modules.utils as utils\n",
    "import importlib\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.cluster.hierarchy import cut_tree\n",
    "from pprint import pprint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "data_dir='../shared_folder'\n",
    "writing_dir='../shared_folder'\n",
    "# writing_dir='../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read file and datasets checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{data_dir}/Bombyx/raw_data/X_long.npz') as f:\n",
    "    long = f['reads']\n",
    "with np.load(f'{data_dir}/Bombyx/raw_data/X_reads.npz') as f:\n",
    "    reads = f['reads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for i in range(len(long)):\n",
    "    assert(len(long[i]) == len(long[i].rstrip()))\n",
    "    total += len(long[i])\n",
    "print(total)\n",
    "total2 = 0\n",
    "for i in range(len(reads)):\n",
    "    assert(len(reads[i]) == len(reads[i].rstrip()))\n",
    "    total2 += len(reads[i])\n",
    "print(total2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{data_dir}/HEK293-ZFAT/raw_data/Control_reads_deduped.npz') as f:\n",
    "    reads = f['reads']\n",
    "print(reads.shape)\n",
    "sequences = utils.remove_reads_with_N(reads, tolerance=0)\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'CENPT'\n",
    "model_name = 'model_inception2'\n",
    "threshold = 0.75\n",
    "with np.load(f'{data_dir}/{data}/results/{model_name}/seqs_{data}_over_{threshold}_with_{model_name}.npz') as f:\n",
    "    reads = f['reads']\n",
    "print(reads.shape)\n",
    "sequences = utils.remove_reads_with_N(reads, tolerance=0)\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = utils.check_read_lengths(reads)\n",
    "dico = dict(sorted(dico.items(), reverse=True))\n",
    "print('{read_length: nb_reads}')\n",
    "print(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{data_dir}/H3K9me3/dataset30M.npz') as f:\n",
    "    reads = f['x_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{data_dir}/HEK293-ZFAT/raw_data/Control_reads_deduped.npz') as f:\n",
    "    reads = f['reads']\n",
    "print(reads.shape)\n",
    "sequences = utils.remove_reads_with_N(reads, tolerance=0)\n",
    "print(len(sequences))\n",
    "\n",
    "print('Looking for read duplicates')\n",
    "dico = utils.find_duplicates(reads, one_hot=False, batch_size=50_000_000, print_freq=20_000_000)\n",
    "values = list(dico.values())\n",
    "print('Looking for duplicate level duplicates')\n",
    "dico2 = utils.find_duplicates(values, batch_size=100_000_000, print_freq=20_000_000)\n",
    "dico2 = dict(sorted(dico2.items(), reverse=True))\n",
    "print('{duplicate_level: nb_unique_reads}')\n",
    "print(dico2)\n",
    "print('top 5 duplicate level reads:', list(dico2.keys())[:5])\n",
    "\n",
    "print('Looking for read duplicates')\n",
    "dico = utils.find_duplicates(sequences, one_hot=False, batch_size=50_000_000, print_freq=20_000_000)\n",
    "values = list(dico.values())\n",
    "print('Looking for duplicate level duplicates')\n",
    "dico2 = utils.find_duplicates(values, batch_size=100_000_000, print_freq=20_000_000)\n",
    "dico2 = dict(sorted(dico2.items(), reverse=True))\n",
    "print('{duplicate_level: nb_unique_reads}')\n",
    "print(dico2)\n",
    "print('top 5 duplicate level reads:', list(dico2.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_reads = [k for k, v in dico.items() if v == 72]\n",
    "print(selected_reads)\n",
    "print(dico[selected_reads[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human telomere sequences\n",
    "seq_list = [\n",
    "    'CTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC',\n",
    "    'CCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAA',\n",
    "    'CCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTA',\n",
    "    'ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCT',\n",
    "    'TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACC',\n",
    "    'AACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCC',\n",
    "    'GGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTA',\n",
    "    'GTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAG',\n",
    "    'TTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGG',\n",
    "    'TAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGG',\n",
    "    'AGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGT',\n",
    "    'GGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTT'\n",
    "]\n",
    "for seq in seq_list:\n",
    "    if seq in dico.keys():\n",
    "        print(dico[seq], seq)\n",
    "    else:\n",
    "        print(0, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(dico.keys())\n",
    "y = list(dico.values())\n",
    "plt.plot(x, y)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('occurences')\n",
    "plt.xlabel('duplicate level')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{data_dir}/H3K27me3/dataset_rel8.npz') as dataset:\n",
    "    y_train = dataset['y_train']\n",
    "    y_valid = dataset['y_valid']\n",
    "    y_test = dataset['y_test']\n",
    "print('total number of reads:',len(y_train) + len(y_valid) + len(y_test))\n",
    "print('train:', len(y_train), 'valid:', len(y_valid), 'test:', len(y_test))\n",
    "print('pos_train:', len(y_train[y_train == 1]), 'pos_valid', len(y_valid[y_valid == 1]), 'pos_test', len(y_test[y_test == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot test distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'CTCF'\n",
    "dataset_name = 'dataset4'\n",
    "model_name = 'model_inception2.2'\n",
    "load_hist = False\n",
    "hist_file = f'distrib_{model_name}_test_hist100000'\n",
    "relabeled = False\n",
    "new_labels = 'dataset_rel8'\n",
    "single_pred_array = True\n",
    "if load_hist:\n",
    "    with np.load(f'{writing_dir}/{data}/results/{model_name}/{hist_file}.npz') as f:\n",
    "        histIP = f['IP']\n",
    "        histControl = f['Control']\n",
    "    bins = np.linspace(0, 1, len(histIP)+1)\n",
    "    win_start = 80000\n",
    "    win_stop = 80100\n",
    "    histIP, histControl, bins = histIP[win_start:win_stop], histControl[win_start:win_stop], bins[win_start:win_stop+1]\n",
    "else:\n",
    "    if single_pred_array:\n",
    "        if relabeled:\n",
    "            with np.load(f'{data_dir}/{data}/results/{model_name}/distrib_{model_name}_test.npz') as f:\n",
    "                preds = f['pred']\n",
    "            with np.load(f'{data_dir}//{data}/{new_labels}.npz') as f:\n",
    "                y_test = f['y_test']\n",
    "        else:\n",
    "            with np.load(f'{data_dir}/{data}/results/{model_name}/distrib_{model_name}_test.npz') as f:\n",
    "                preds = f['pred']\n",
    "            with np.load(f'{data_dir}/{data}/{dataset_name}.npz') as f:\n",
    "                y_test = f['y_test']\n",
    "        predIP = preds[y_test == 1]\n",
    "        predControl = preds[y_test == 0]\n",
    "    else:\n",
    "        with np.load(f'{data_dir}/{data}/results/distrib_{model_name}_test.npz') as f:\n",
    "                predIP = f['predIP']\n",
    "                predControl = f['predControl']\n",
    "    accuracy = (len(predIP[predIP > 0.5]) + len(predControl[predControl < 0.5])) / (len(predIP) + len(predControl))\n",
    "    n_bins = 100\n",
    "    histIP, bins = np.histogram(predIP, bins=n_bins, range=(0, 1))\n",
    "    histControl = np.histogram(predControl, bins=n_bins, range=(0, 1))[0]\n",
    "\n",
    "widths = [5, 1]\n",
    "heights = [1.2] + [2.5]*2\n",
    "fig = plt.figure(figsize=(np.sum(widths), np.sum(heights)))\n",
    "# fig.suptitle(f\"Distribution of predictions of {model_name} on {data} for IP and Control reads\")\n",
    "gs = gridspec.GridSpec(ncols=len(widths), nrows=len(heights), width_ratios=widths, height_ratios=heights)\n",
    "\n",
    "ax0_top = plt.subplot(gs[0, 0])\n",
    "label_col0 = 'predictions'\n",
    "ax0_top.annotate(label_col0, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax0_top.axis('off')\n",
    "\n",
    "ax1_top = plt.subplot(gs[0, 1])\n",
    "label_col1 = 'scale'\n",
    "ax1_top.annotate(label_col1, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax1_top.axis('off')\n",
    "\n",
    "for nrow in range(1,3):\n",
    "\n",
    "    ax0 = plt.subplot(gs[nrow, 0])\n",
    "\n",
    "    ax0.bar(bins[:-1], histIP, width=np.diff(bins), label='IP', alpha=0.5)\n",
    "    ax0.bar(bins[:-1], histControl, width=np.diff(bins), label='Control', alpha=0.5)\n",
    "\n",
    "    if not load_hist:\n",
    "        ax0.axvline(x=0.5, color='black', label='t0.5')\n",
    "        thres = 0.9\n",
    "        ax0.axvline(x=thres, color='red', label=f't{thres}')\n",
    "    \n",
    "    ax1 = plt.subplot(gs[nrow, 1])\n",
    "    if nrow == 1:\n",
    "        label = 'linear'\n",
    "        if not load_hist:\n",
    "            ax1.annotate(f'accuracy: {round(accuracy, 4)}', (0, 0.4), xycoords='axes fraction', va='center', fontsize=13)\n",
    "    elif nrow == 2:\n",
    "        label = 'log'\n",
    "        ax0.set_yscale('log')\n",
    "    ax1.annotate(label, (0.1, 0.6), xycoords='axes fraction', va='center', fontsize=18)\n",
    "    ax1.axis('off')\n",
    "ax0.set_xlabel(\"predictions\", fontsize=18)\n",
    "ax0.set_ylabel(\"read count\", fontsize=18)\n",
    "handles, labels = ax0.get_legend_handles_labels()\n",
    "ax0_top.legend(handles, labels, loc='right')\n",
    "\n",
    "if not os.path.isdir(f'{writing_dir}/{data}/results/{model_name}'):\n",
    "    os.makedirs(f'{writing_dir}/{data}/results/{model_name}')\n",
    "fig_start = f'{writing_dir}/{data}/results/{model_name}/distrib_{model_name}_test'\n",
    "fig_name = fig_start + '.png'\n",
    "fig_dups = 0\n",
    "while os.path.exists(fig_name):\n",
    "    fig_dups += 1\n",
    "    fig_name = fig_start + f'({fig_dups}).png'\n",
    "plt.savefig(fig_name, bbox_inches='tight')\n",
    "if not load_hist:\n",
    "    np.savez(f'{writing_dir}/{data}/results/{model_name}/distrib_{model_name}_test_hist{n_bins}', IP=histIP, Control=histControl)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{data_dir}/{data}/{dataset_name}.npz') as f:\n",
    "    x_test = f['x_test']\n",
    "thres = 0.9\n",
    "selected_reads = x_test[preds.ravel() > thres, :, :]\n",
    "print(f'nb of selected reads with thres {thres}:', len(selected_reads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{data_dir}/{data}/{dataset_name}.npz') as f:\n",
    "    x_test = f['x_test']\n",
    "thres0 = 0.83\n",
    "thres1 = 0.99\n",
    "preds = preds.ravel()\n",
    "mask = np.logical_and(preds > thres0, preds < thres1)\n",
    "selected_reads = x_test[mask, :, :]\n",
    "print(f'nb of selected reads between {thres0} and {thres1}:', len(selected_reads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write into fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3m per 10_000_000 seqs of 101 bp\n",
    "utils.write_fasta(utils.one_hot_to_seq(selected_reads), f'{writing_dir}/{data}/results/{model_name}/seqs_{data}_over_{thres}_with_{model_name}.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3m per 10_000_000 seqs of 101 bp\n",
    "utils.write_fasta(utils.one_hot_to_seq(selected_reads), f'{writing_dir}/{data}/results/{model_name}/seqs_{data}_over_{thres0}_below_{thres1}_with_{model_name}.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(f'{writing_dir}/{data}/results/{model_name}'):\n",
    "    os.mkdir(f'{writing_dir}/{data}/results/{model_name}/')\n",
    "logs = pd.read_csv(f'{data_dir}/{data}/Trainedmodels/{model_name}/epoch_data.csv')\n",
    "fig = plt.figure(figsize=(5, 6))\n",
    "fig.suptitle(f'Training logs for {model_name} on {data}')\n",
    "ax0 = plt.subplot(211)\n",
    "ax0.plot(logs['val_accuracy'], label='valid')\n",
    "ax0.plot(logs['accuracy'], label='train')\n",
    "ax0.set_ylabel('accuracy')\n",
    "ax0.legend()\n",
    "\n",
    "ax1 = plt.subplot(212)\n",
    "ax1.plot(logs['val_loss'], label='valid')\n",
    "ax1.plot(logs['loss'], label='train')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "plt.legend()\n",
    "plt.savefig(f'{writing_dir}/{data}/results/{model_name}/{model_name}_train_log.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_accuracy = np.size(predIP[predIP > 0.5]) / np.size(predIP)\n",
    "Control_accuracy = (np.size(predControl[predControl < 0.5])\n",
    "                    / np.size(predControl))\n",
    "print('accuracy: ', (IP_accuracy + Control_accuracy) / 2)\n",
    "print('IP accuracy: ', IP_accuracy)\n",
    "print('Control accuracy: ', Control_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot test distribution during epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'H3K27ac'\n",
    "dataset_name = 'dataset4'\n",
    "model_name = 'model_inception2.2_rel8'\n",
    "new_labels = 'dataset_rel8'\n",
    "relabeled = True\n",
    "preds = np.load(f'{data_dir}/{data}/Trainedmodels/{model_name}/eval_epochs.npy')\n",
    "# Watching for nan predictions\n",
    "# for epoch in range(preds.shape[0]):\n",
    "#     nans = np.count_nonzero(np.isnan(preds[epoch]))\n",
    "#     print(f'Epoch {epoch}: {nans} nan predictions')\n",
    "if relabeled:\n",
    "    with np.load(f'{data_dir}/{data}/{new_labels}.npz') as f:\n",
    "        y_test = f['y_test']\n",
    "else:\n",
    "    with np.load(f'{data_dir}//{data}/{dataset_name}.npz') as f:\n",
    "        y_test = f['y_test']\n",
    "\n",
    "\n",
    "predsIP = preds[:, y_test == 1]\n",
    "predsControl = preds[:, y_test == 0]\n",
    "correct_IP = (predsIP > 0.5)\n",
    "correct_Control = (predsControl < 0.5)\n",
    "accuracys = (np.sum(correct_IP, axis=1) + np.sum(correct_Control, axis=1)) / len(y_test)\n",
    "amax = np.argmax(accuracys)\n",
    "print(amax)\n",
    "print(accuracys[amax])\n",
    "plt.plot(accuracys)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# epochs = range(0, preds.shape[0], 5)\n",
    "epochs = [0, 1, 2, 3, 4, 9, 14, 19]\n",
    "widths = [5, 1]\n",
    "heights = [1.2] + [2]*len(epochs)\n",
    "fig = plt.figure(figsize=(np.sum(widths), np.sum(heights)))\n",
    "# fig.suptitle(f\"Distribution of predictions of {model_name} during epochs on {data} for IP and Control reads\")\n",
    "gs = gridspec.GridSpec(ncols=len(widths), nrows=len(heights), width_ratios=widths, height_ratios=heights)\n",
    "\n",
    "ax0_top = plt.subplot(gs[0, 0])\n",
    "label_col0 = 'prediction'\n",
    "ax0_top.annotate(label_col0, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax0_top.axis('off')\n",
    "\n",
    "ax1_top = plt.subplot(gs[0, 1])\n",
    "label_col1 = 'epoch'\n",
    "ax1_top.annotate(label_col1, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax1_top.axis('off')\n",
    "\n",
    "nrow = 1\n",
    "for epoch in epochs:\n",
    "    ax0 = plt.subplot(gs[nrow, 0])\n",
    "    ax0.hist(predsIP[epoch], bins=100, label='IP', alpha=0.5, range=(0,1))\n",
    "    ax0.hist(predsControl[epoch], bins=100, label='Control', alpha=0.5, range=(0,1))\n",
    "    ax0.axvline(x=0.5, color='black', label='base thres')\n",
    "    ax0.set_yscale('log')\n",
    "\n",
    "    ax1 = plt.subplot(gs[nrow, 1])\n",
    "    label = f'{epoch+1}'\n",
    "    ax1.annotate(label, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "    ax1.annotate(f'accuracy: {round(accuracys[epoch], 4)}', (0, 0.4), xycoords='axes fraction', va='center', fontsize=13)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    nrow += 1\n",
    "\n",
    "handles, labels = ax0.get_legend_handles_labels()\n",
    "ax0_top.legend(handles, labels, loc='right')\n",
    "ax0.set_xlabel(\"predictions\")\n",
    "ax0.set_ylabel(\"read count\")\n",
    "if not os.path.isdir(f'{writing_dir}/{data}/results/{model_name}'):\n",
    "    os.makedirs(f'{writing_dir}/{data}/results/{model_name}/')\n",
    "plt.savefig(f'{writing_dir}/{data}/results/{model_name}/distrib_{model_name}_test_epochs.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare predictions between 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'H3K27ac'\n",
    "dataset_name = 'dataset4'\n",
    "model_name = 'model_inception2.2_rel8'\n",
    "model_name2 = 'model_inception2.2'\n",
    "new_labels = 'dataset_rel8'\n",
    "relabeled = True\n",
    "preds = np.load(f'{data_dir}/{data}/Trainedmodels/{model_name}/eval_epochs.npy')\n",
    "preds2 = np.load(f'{data_dir}/{data}/Trainedmodels/{model_name2}/eval_epochs.npy')\n",
    "min_len = min(len(preds), len(preds2))\n",
    "diff = preds[:min_len] - preds2[:min_len]\n",
    "\n",
    "if relabeled:\n",
    "    with np.load(f'{data_dir}/{data}/{new_labels}.npz') as f:\n",
    "        y_test = f['y_test']\n",
    "else:\n",
    "    with np.load(f'{data_dir}//{data}/{dataset_name}.npz') as f:\n",
    "        y_test = f['y_test']\n",
    "\n",
    "diffIP = diff[:, y_test == 1]\n",
    "diffControl = diff[:, y_test == 0]\n",
    "\n",
    "# epochs = range(0, preds.shape[0], 5)\n",
    "epochs = [0, 1, 2, 3, 4, 9, 14, 19]\n",
    "widths = [5, 1]\n",
    "heights = [1.2] + [2]*len(epochs)\n",
    "fig = plt.figure(figsize=(np.sum(widths), np.sum(heights)))\n",
    "# fig.suptitle(f\"Distribution of prediction differences of {model_name} during epochs on {data} for IP and Control reads\")\n",
    "gs = gridspec.GridSpec(ncols=len(widths), nrows=len(heights), width_ratios=widths, height_ratios=heights)\n",
    "\n",
    "ax0_top = plt.subplot(gs[0, 0])\n",
    "label_col0 = 'prediction difference'\n",
    "ax0_top.annotate(label_col0, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax0_top.axis('off')\n",
    "\n",
    "ax1_top = plt.subplot(gs[0, 1])\n",
    "label_col1 = 'epoch'\n",
    "ax1_top.annotate(label_col1, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax1_top.axis('off')\n",
    "\n",
    "nrow = 1\n",
    "for epoch in epochs:\n",
    "    ax0 = plt.subplot(gs[nrow, 0])\n",
    "    ax0.hist(diffIP[epoch], bins=100, label='IP', alpha=0.5, range=(-1, 1))\n",
    "    ax0.hist(diffControl[epoch], bins=100, label='Control', alpha=0.5, range=(-1, 1))\n",
    "    # ax0.set_yscale('log')\n",
    "\n",
    "    ax1 = plt.subplot(gs[nrow, 1])\n",
    "    label = f'{epoch+1}'\n",
    "    ax1.annotate(label, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    nrow += 1\n",
    "\n",
    "handles, labels = ax0.get_legend_handles_labels()\n",
    "ax0_top.legend(handles, labels, loc='right')\n",
    "ax0.set_xlabel(\"difference\")\n",
    "ax0.set_ylabel(\"read count\")\n",
    "if not os.path.isdir(f'{writing_dir}/{data}/results/{model_name}'):\n",
    "    os.mkdir(f'{writing_dir}/{data}/results/{model_name}/')\n",
    "plt.savefig(f'{writing_dir}/{data}/results/{model_name}/distrib_{model_name}_vs_{model_name2}_test_epochs.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot GC_content distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='HEK293-ZFAT'\n",
    "dataset_name='dataset4'\n",
    "with np.load(f'{data_dir}/{data}/{dataset_name}.npz') as f:\n",
    "    x_test = f['x_test']\n",
    "    y_test = f['y_test']\n",
    "gc_content = utils.GC_content(x_test)\n",
    "\n",
    "bins = np.histogram(gc_content, bins=100, range=(0, 1))[1]\n",
    "accuracy, thres = utils.classify_1D(gc_content, y_test, bins=100)\n",
    "print('accuracy :', accuracy)\n",
    "\n",
    "widths = [5, 1]\n",
    "heights = [1.2] + [2.5]*2\n",
    "fig = plt.figure(figsize=(np.sum(widths), np.sum(heights)))\n",
    "# fig.suptitle(f\"Distribution of predictions of {model_name} on {data} for IP and Control reads\")\n",
    "gs = gridspec.GridSpec(ncols=len(widths), nrows=len(heights), width_ratios=widths, height_ratios=heights)\n",
    "\n",
    "ax0_top = plt.subplot(gs[0, 0])\n",
    "label_col0 = 'GC content'\n",
    "ax0_top.annotate(label_col0, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax0_top.axis('off')\n",
    "\n",
    "ax1_top = plt.subplot(gs[0, 1])\n",
    "label_col1 = 'scale'\n",
    "ax1_top.annotate(label_col1, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax1_top.axis('off')\n",
    "\n",
    "for nrow in range(1,3):\n",
    "\n",
    "    ax0 = plt.subplot(gs[nrow, 0])\n",
    "    ax0.hist(gc_content[y_test == 1], bins, label='IP', alpha=0.5)\n",
    "    ax0.hist(gc_content[y_test == 0], bins, label='Control', alpha=0.5)\n",
    "    ax0.axvline(x=thres, color='black', label=f't{round(thres, 2)}')\n",
    "    \n",
    "    ax1 = plt.subplot(gs[nrow, 1])\n",
    "    if nrow == 1:\n",
    "        scale_label = 'linear'\n",
    "        ax1.annotate(f'accuracy: {round(accuracy, 4)}', (0, 0.4), xycoords='axes fraction', va='center', fontsize=13)\n",
    "    elif nrow == 2:\n",
    "        scale_label = 'log'\n",
    "        ax0.set_yscale('log')\n",
    "    ax1.annotate(scale_label, (0.1, 0.6), xycoords='axes fraction', va='center', fontsize=18)\n",
    "    ax1.axis('off')\n",
    "ax0.set_xlabel(\"GC content\", fontsize=18)\n",
    "ax0.set_ylabel(\"read count\", fontsize=18)\n",
    "handles, labels = ax0.get_legend_handles_labels()\n",
    "ax0_top.legend(handles, labels, loc='right')\n",
    "\n",
    "fig.suptitle(f'Distribution of GC content in {data} reads', fontsize=18)\n",
    "plt.savefig(f'{writing_dir}/{data}/results/gc_content_classification_{dataset_name}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "chr_id = 1\n",
    "# load one_hot_encoded sequence\n",
    "chr_file = f'../shared_folder/Human/assembly/chr{chr_id}.npz'\n",
    "with np.load(chr_file) as f:\n",
    "    one_hot_chr = f['one_hot_genome']\n",
    "window_size = 101\n",
    "valid_window_idx = utils.remove_windows_with_N(one_hot_chr, window_size)\n",
    "print('All windows:', len(one_hot_chr)-window_size+1)\n",
    "print('Without N:', len(valid_window_idx))\n",
    "\n",
    "# # load sequence with soft masked repeats\n",
    "# with np.load(f'{data_dir}/Human/assembly/chr{chr_id}_seq.npz') as f:\n",
    "#     chr_seq = f['reads'][0]\n",
    "# chr_seq = np.array(list(chr_seq))\n",
    "# repeats_idx, = np.where(chr_seq > 'Z')\n",
    "# print(len(repeats_idx), 'bases in repeats')\n",
    "\n",
    "# load repeat annotation\n",
    "repeats = utils.parse_repeats(f'{data_dir}/Human/annotations/repeats/hg38.fa.out', window_size)\n",
    "chr_repeats = repeats[f'{chr_id}']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'CTCF'\n",
    "model_name = 'model_inception2.2'\n",
    "model_name1 = 'model_inception2.2.rep1'\n",
    "model_name_rel = 'model_inception2.2_rel8'\n",
    "pred_on_genome = False\n",
    "relabeled = False\n",
    "if pred_on_genome:\n",
    "    with np.load(f'{data_dir}/{data}/results/{model_name}/preds_on_genome.npz') as f:\n",
    "        preds0 = f[f'chr{chr_id}']\n",
    "    with np.load(f'{data_dir}/{data}/results/{model_name1}/preds_on_genome.npz') as f:\n",
    "        preds1 = f[f'chr{chr_id}']\n",
    "    if relabeled:\n",
    "        with np.load(f'{data_dir}/{data}/results/{model_name_rel}/preds_on_genome.npz') as f:\n",
    "            preds_rel = f[f'chr{chr_id}']\n",
    "else:\n",
    "    with np.load(f'{data_dir}/{data}/results/{model_name}/preds_on_chr{chr_id}.npz') as f:\n",
    "        preds0 = f['preds']\n",
    "    with np.load(f'{data_dir}/{data}/results/{model_name1}/preds_on_chr{chr_id}.npz') as f:\n",
    "        preds1 = f['preds']\n",
    "    if relabeled:\n",
    "        with np.load(f'{data_dir}/{data}/results/{model_name_rel}/preds_on_chr{chr_id}.npz') as f:\n",
    "            preds_rel = f['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "# z-score, mean and smooth\n",
    "preds = (preds0 + preds1)/2  # mean\n",
    "smooth_size = window_size\n",
    "# smooth = utils.smooth(preds, smooth_size, mode='linear')\n",
    "z_score_preds = utils.z_score(preds, valid_window_idx)\n",
    "# z_score_preds0 = utils.z_score(preds0, valid_window_idx)\n",
    "# z_score_preds1 = utils.z_score(preds1, valid_window_idx)\n",
    "# z_score_preds_rel = utils.z_score(preds_rel, valid_window_idx)\n",
    "# Smooth mean\n",
    "smooth_size = window_size\n",
    "z_score_smooth = utils.smooth(z_score_preds, smooth_size, mode='linear')\n",
    "# z_score_smooth_rel = utils.smooth(z_score_preds_rel, smooth_size, mode='linear')\n",
    "\n",
    "# # mean and std with or without windows containing Ns\n",
    "# mean = np.mean(preds)\n",
    "# std = np.std(preds)\n",
    "# rel_preds = preds[valid_window_idx]\n",
    "# rel_mean = np.mean(rel_preds)\n",
    "# rel_std = np.std(rel_preds)\n",
    "# print('mean and std for all windows:', mean, std)\n",
    "# print('mean and std without Ns:', rel_mean, rel_std)\n",
    "\n",
    "# # correlations between replicas\n",
    "# inter_corr = pearsonr(preds0, preds1)\n",
    "# print(inter_corr, 'correlation rep0-rep1')\n",
    "# corr_mrel = pearsonr(preds, preds_rel)\n",
    "# print(corr_mrel, 'correlation mean-relabeled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load peaks, logFC and p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "files = {\n",
    "    'CTCF': {\n",
    "        'peaks': 'ENCFF748YHT.bed',\n",
    "        'p_value': 'ENCFF701JJR.bigWig',\n",
    "        'logFC': 'ENCFF902EIC.bigWig',\n",
    "    },\n",
    "    'H3K27ac': {\n",
    "        'peaks': 'ENCFF553YTG.bed',\n",
    "        'p_value': 'ENCFF472QZS.bigWig',\n",
    "        'logFC': 'ENCFF408XBB.bigWig',\n",
    "    },\n",
    "    'H3K27me3': {\n",
    "        'peaks': 'ENCFF968KLE.bed',\n",
    "        'p_value': 'ENCFF805LJE.bigWig',\n",
    "        'logFC': 'ENCFF463LIU.bigWig',\n",
    "    },\n",
    "    'H3K9me3': {\n",
    "        'peaks': 'ENCFF411TKR.bed',\n",
    "        'p_value': 'ENCFF042EJN.bigWig',\n",
    "        'logFC': 'ENCFF088IKU.bigWig',\n",
    "    },\n",
    "}\n",
    "peaks_dict = utils.parse_bed_peaks(f'{data_dir}/{data}/raw_data/peaks/{files[data][\"peaks\"]}', window_size)\n",
    "p_vals = utils.load_annotation(f'{data_dir}/{data}/raw_data/p_value/{files[data][\"p_value\"]}', chr_id, window_size)\n",
    "log_fc = utils.load_annotation(f'{data_dir}/{data}/raw_data/logFC/{files[data][\"logFC\"]}', chr_id, window_size)\n",
    "\n",
    "# other_data = 'H3K27ac'\n",
    "# other_peaks_dict = utils.parse_bed_peaks(f'{data_dir}/{other_data}/raw_data/peaks/{files[other_data][\"peaks\"]}', window_size)\n",
    "# other_p_vals = utils.load_annotation(f'{data_dir}/{other_data}/raw_data/p_value/{files[other_data][\"p_value\"]}', chr_id, window_size)\n",
    "# other_log_fc = utils.load_annotation(f'{data_dir}/{other_data}/raw_data/logFC/{files[other_data][\"logFC\"]}', chr_id, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total peaks and only high ones\n",
    "# adjust start and end to sliding window index\n",
    "chr_peaks = peaks_dict[f'{chr_id}']\n",
    "print(chr_peaks.shape, f'total peaks in {data}')\n",
    "high_peaks = chr_peaks[chr_peaks[:,2] >= 1000]\n",
    "print(high_peaks.shape, f'high peaks in {data}')\n",
    "# # Peaks of different dataset\n",
    "# other_chr_peaks = other_peaks_dict['1']\n",
    "# print(other_chr_peaks.shape, f'total peaks in {other_data}')\n",
    "# other_high_peaks = other_chr_peaks[other_chr_peaks[:,2] >= 1000]\n",
    "# print(other_high_peaks.shape, f'high peaks in {other_data}')\n",
    "\n",
    "# # Correlation between fold change and predicitons\n",
    "# corr_mean = pearsonr(z_score_preds, log_fc)\n",
    "# print(corr_mean, 'correlation mean-logFC')\n",
    "# corr_smooth = pearsonr(z_score_smooth, log_fc)\n",
    "# print(corr_smooth, 'correlation smooth-logFC')\n",
    "# corr_rel = pearsonr(z_score_smooth_rel, log_fc)\n",
    "# print(corr_rel, 'correlation rel-logFC')\n",
    "\n",
    "# # Correlation with other dataset\n",
    "# corr_FC = pearsonr(log_fc, other_log_fc)\n",
    "# print(corr_FC, f'correlation logFC {data}-{other_data}')\n",
    "# corr_p_val = pearsonr(p_vals, other_p_vals)\n",
    "# print(corr_p_val, f'correlation p_vals {data}-{other_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between p-value and predicitons\n",
    "corr_mean = pearsonr(z_score_preds, p_vals)\n",
    "print(corr_mean, 'correlation mean-p_value')\n",
    "# corr_smooth = pearsonr(z_score_smooth, p_vals)\n",
    "# print(corr_smooth, 'correlation smooth-p_value')\n",
    "# corr_rel = pearsonr(z_score_smooth_rel, p_vals)\n",
    "# print(corr_rel, 'correlation rel-p_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Distribution of predictions on all windows\n",
    "# plt.hist(preds, bins=100, range=(0, 1))\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "# # Distribution of predictions on N free windows\n",
    "# plt.hist(preds[valid_window_idx], bins=100, range=(0, 1))\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "# # Same for z-score\n",
    "# plt.hist(z_score_preds, bins=100)\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "plt.hist(z_score_smooth[valid_window_idx], bins=100, label='IP signal')\n",
    "# plt.yscale('log')\n",
    "plt.axvline(x=5, color='red', label='t5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of peak lengths\n",
    "peak_lengths = np.diff(chr_peaks[:, :2], axis=1).ravel()\n",
    "high_peaks_lengths = np.diff(high_peaks[:, :2], axis=1).ravel()\n",
    "plt.hist(peak_lengths, bins=100, range=(0, 1500))\n",
    "plt.show()\n",
    "plt.close()\n",
    "# plt.hist(high_peaks_lengths, bins=100)\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "# # Distribution of peak scores\n",
    "# plt.hist(chr_peaks[:, 2], bins=100)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTCF_strange_peaks = other_chr_peaks[peak_lengths == 810]\n",
    "# print(CTCF_strange_peaks.shape, 'strange peaks')\n",
    "# strange_peaks_indices = CTCF_strange_peaks[:, :1] + np.expand_dims(np.arange(810), axis=0)\n",
    "# strange_peaks_one_hot = one_hot_chr[strange_peaks_indices]\n",
    "# strange_peaks_seq = utils.one_hot_to_seq(strange_peaks_one_hot)\n",
    "# utils.write_fasta(strange_peaks_seq, f'{data_dir}/CTCF/raw_data/peaks/strange_peaks_seqs.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metaplot of predictions over mid peaks\n",
    "importlib.reload(utils)\n",
    "mid_peaks = (chr_peaks[:, 0] + chr_peaks[:, 1]) // 2\n",
    "window_half_size = 600\n",
    "clust, means, window = utils.metaplot_over_indices(z_score_preds, mid_peaks, window_half_size, plot='clustermap', res_dir=f'{writing_dir}/{data}/results/{model_name}', data=data, chr=f'chr{chr_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metaplot of predictions over mid peaks\n",
    "importlib.reload(utils)\n",
    "mid_peaks = (chr_peaks[:, 0] + chr_peaks[:, 1]) // 2\n",
    "window_half_size = 600\n",
    "corrs, means, window = utils.metaplot_over_indices(z_score_preds, mid_peaks, window_half_size, plot='heatmap', res_dir=f'{writing_dir}/{data}/results/{model_name}', data=data, chr=f'chr{chr_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "cluster_idx = cut_tree(clust.dendrogram_row.linkage, n_clusters).ravel()\n",
    "clusters = [np.where(cluster_idx == i)[0] for i in range(n_clusters)]\n",
    "\n",
    "clust_order = np.array(clust.dendrogram_row.reordered_ind)\n",
    "samples = [np.where(clust_order == cluster[0])[0] for cluster in clusters]\n",
    "print('cluster indices ordered from top to bottom', np.argsort(samples, axis=0).ravel())\n",
    "\n",
    "mean_order = np.argsort(corrs)[::-1]\n",
    "cluster_idx_mean_order = cluster_idx[mean_order]\n",
    "plt.scatter(np.arange(len(mean_order)), cluster_idx_mean_order, marker='.')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of predictions over chosen peak\n",
    "i = 0\n",
    "peak_start, peak_end, score = chr_peaks[i]\n",
    "print((peak_start, peak_end, score))\n",
    "window = np.arange(peak_start-2000, peak_end+2000)\n",
    "# both replicas\n",
    "plt.plot(window, z_score_preds0[window], label='prediction rep0', alpha=0.5)\n",
    "plt.plot(window, z_score_preds1[window], label='prediction rep1', alpha=0.5)\n",
    "plt.plot([peak_start, peak_end], [2, 2], marker = 'o', label='ENCODE peak')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "plt.close()\n",
    "# mean vs smooth mean\n",
    "plt.plot(window, z_score_preds[window], label='mean prediction')\n",
    "plt.plot(window, z_score_smooth[window], label='smooth mean prediction')\n",
    "plt.plot([peak_start, peak_end], [2, 2], marker = 'o', label='ENCODE peak')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "# Find peaks in prediction with custom function and plot lengths\n",
    "pred_thres = 1.5\n",
    "pred_peaks = utils.find_peaks(z_score_smooth, pred_thres=pred_thres, length_thres=5, tol=0)\n",
    "lengths = np.sort(np.diff(pred_peaks, axis=1).ravel())\n",
    "distances = np.sort(pred_peaks[1:, 0] - pred_peaks[:-1, 1])  # check distances between peaks\n",
    "print(pred_peaks.shape[0], 'peaks found')\n",
    "MAX = 300\n",
    "plt.hist(lengths, bins=100, range=(-0.5, MAX+0.5))\n",
    "# plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "# find overlaps between predicted peaks and ENCODE peaks\n",
    "(chr_overlap, pred_overlap), (chr_non_overlap, pred_non_overlap) = utils.overlapping_peaks(chr_peaks, pred_peaks)\n",
    "print(len(chr_overlap), 'chr peaks overlap with pred peaks')\n",
    "print(len(pred_overlap), 'pred peaks overlap with chr peaks')\n",
    "print(len(chr_non_overlap), 'chr peaks do not overlap')\n",
    "print(len(pred_non_overlap), 'pred peaks do not overlap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted and encode peaks on predicted signal\n",
    "importlib.reload(utils)\n",
    "i = 13\n",
    "peak_start, peak_end, *score = chr_peaks[i]\n",
    "print((peak_start, peak_end, *score))\n",
    "window = np.arange(peak_start-2000, peak_end+2000)\n",
    "pred_peaks_to_show = utils.find_peaks_in_window(pred_peaks, peak_start-2000, peak_end+2000)\n",
    "chr_peaks_to_show = utils.find_peaks_in_window(chr_peaks, peak_start-2000, peak_end+2000)\n",
    "\n",
    "# plot peaks and smooth mean\n",
    "ax1 = plt.subplot()\n",
    "ax1.plot(window, z_score_smooth[window], label='smooth prediction')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(window, p_vals[window], label='p_value', alpha=0.5, color='brown')\n",
    "for id, (start, end, *_) in enumerate(pred_peaks_to_show):\n",
    "    if id == 0:\n",
    "        ax1.plot([start, end], [pred_thres, pred_thres], marker = '.', color='red', label='predicted peaks')\n",
    "    else:\n",
    "        ax1.plot([start, end], [pred_thres, pred_thres], marker = '.', color='red')\n",
    "for id, (start, end, *_) in enumerate(chr_peaks_to_show):\n",
    "    if id == 0:\n",
    "        ax1.plot([start, end], [pred_thres-1, pred_thres-1], marker = '.', color='black', label='ENCODE peaks')\n",
    "    else:\n",
    "        ax1.plot([start, end], [pred_thres-1, pred_thres-1], marker = '.', color='black')\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "plt.legend(handles1 + handles2, labels1 + labels2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediction signal vs fold change and p-value\n",
    "# i = 3\n",
    "peak_start, peak_end, *score = pred_non_overlap[i]\n",
    "print(peak_start, peak_end, *score)\n",
    "window = np.arange(peak_start-2000, peak_end+2000)\n",
    "ax1 = plt.subplot()\n",
    "ax1.plot(window, z_score_smooth[window], label='prediction', alpha=0.5)\n",
    "ax1.plot([peak_start, peak_end], [pred_thres, pred_thres], marker = 'o', label='ENCODE peak')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(window, p_vals[window], label='p_value', alpha=0.5, color='black')\n",
    "ax3 = ax1.twinx()\n",
    "ax3.plot(window, log_fc[window], label='logFC', alpha=0.5, color='red')\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "handles3, labels3 = ax3.get_legend_handles_labels()\n",
    "plt.legend(handles1 + handles2 + handles3, labels1 + labels2 + labels3, loc='lower right')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chr_repeats.keys())\n",
    "all_chr_repeats = np.array([rep for array in chr_repeats.values() for rep in array])\n",
    "lengths = np.diff(all_chr_repeats, axis=1).ravel()\n",
    "print(np.sum(lengths), 'bases in repeats')\n",
    "print(len(one_hot_chr), f'bases in chr{chr_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_peaks_in_repeats = {}\n",
    "for family in chr_repeats.keys():\n",
    "    (pred_peaks_in_repeats[family], _), _ = utils.overlapping_peaks(pred_peaks, chr_repeats[family])\n",
    "chr_peaks_in_repeats = {}\n",
    "for family in chr_repeats.keys():\n",
    "    (chr_peaks_in_repeats[family], _), _ = utils.overlapping_peaks(chr_peaks, chr_repeats[family])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_families = 0\n",
    "for family in pred_peaks_in_repeats.keys():\n",
    "    if '?' in family:\n",
    "        print(family, len(pred_peaks_in_repeats[family]))\n",
    "        n_families += 1\n",
    "print(n_families, 'families')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00e7424330818c90ac25a9eb217b83ab19236ec3c884b7e4b78b97ecfb399601"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf2.5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
