{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'shared_folder.Modules.utils' from '/home/alex/shared_folder/Modules/utils.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.axes as ax \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import io\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "import pandas as pd\n",
    "import shared_folder.Modules.utils as utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read file and datasets checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('CENPT/raw_data/IP_reads.npz') as f:\n",
    "    reads = f['reads']\n",
    "print(reads.shape)\n",
    "sequences = utils.remove_reads_with_N(reads, tolerance=0)\n",
    "print(len(sequences))\n",
    "sequences10 = utils.remove_reads_with_N(reads, tolerance=10)\n",
    "print(len(sequences10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = utils.check_read_lengths(reads)\n",
    "dico = dict(sorted(dico.items(), reverse=True))\n",
    "print('{read_length: nb_reads}')\n",
    "print(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('shared_folder/CENPT/dataset3.npz') as f:\n",
    "    reads = f['x_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for read duplicates\n",
      "Decoding batch 0\n",
      "seq 1000000/5000000 duplicates\n",
      "seq 2000000/5000000 duplicates\n",
      "seq 3000000/5000000 duplicates\n",
      "seq 4000000/5000000 duplicates\n",
      "seq 5000000/5000000 duplicates\n",
      "Decoding batch 1\n",
      "seq 1000000/5000000 duplicates\n",
      "seq 2000000/5000000 duplicates\n",
      "seq 3000000/5000000 duplicates\n",
      "seq 4000000/5000000 duplicates\n",
      "seq 5000000/5000000 duplicates\n",
      "Looking for duplicate level duplicates\n",
      "{duplicate_level: nb_unique_reads}\n",
      "{633: 1, 573: 1, 568: 1, 492: 1, 476: 1, 474: 1, 438: 1, 418: 1, 407: 1, 403: 1, 401: 1, 364: 1, 353: 2, 351: 1, 347: 1, 344: 2, 340: 1, 331: 1, 311: 1, 305: 1, 301: 1, 296: 1, 283: 1, 281: 1, 279: 1, 275: 1, 271: 1, 270: 1, 267: 1, 246: 1, 242: 2, 241: 1, 239: 1, 237: 1, 236: 1, 233: 2, 232: 2, 231: 1, 229: 2, 225: 1, 224: 1, 223: 1, 222: 3, 221: 1, 219: 2, 216: 1, 215: 1, 212: 1, 211: 3, 206: 2, 205: 2, 204: 1, 203: 1, 201: 2, 200: 6, 199: 2, 198: 1, 196: 1, 195: 1, 194: 3, 193: 1, 189: 1, 187: 3, 186: 2, 185: 1, 184: 3, 182: 3, 181: 1, 180: 1, 178: 2, 176: 2, 175: 1, 174: 1, 173: 2, 172: 3, 171: 4, 170: 1, 169: 3, 168: 3, 167: 1, 164: 3, 163: 1, 161: 2, 160: 4, 159: 2, 158: 2, 157: 2, 156: 1, 155: 2, 154: 4, 153: 2, 151: 1, 150: 2, 148: 2, 147: 3, 145: 3, 144: 5, 143: 1, 142: 2, 141: 3, 140: 5, 139: 5, 138: 2, 137: 3, 136: 3, 135: 1, 134: 5, 133: 4, 132: 7, 131: 3, 130: 7, 129: 5, 128: 2, 127: 3, 126: 1, 125: 2, 124: 1, 123: 4, 122: 4, 121: 4, 120: 3, 119: 8, 118: 4, 117: 5, 116: 3, 115: 3, 114: 5, 113: 4, 112: 5, 111: 2, 110: 3, 109: 4, 108: 13, 107: 12, 106: 9, 105: 3, 104: 4, 103: 6, 102: 9, 101: 8, 100: 3, 99: 7, 98: 5, 97: 10, 96: 4, 95: 7, 94: 11, 93: 4, 92: 11, 91: 8, 90: 14, 89: 7, 88: 12, 87: 9, 86: 8, 85: 10, 84: 10, 83: 13, 82: 7, 81: 15, 80: 8, 79: 17, 78: 17, 77: 9, 76: 16, 75: 17, 74: 4, 73: 17, 72: 10, 71: 19, 70: 16, 69: 18, 68: 20, 67: 17, 66: 18, 65: 24, 64: 14, 63: 22, 62: 20, 61: 19, 60: 30, 59: 29, 58: 32, 57: 23, 56: 26, 55: 26, 54: 26, 53: 40, 52: 38, 51: 29, 50: 24, 49: 34, 48: 62, 47: 41, 46: 33, 45: 65, 44: 50, 43: 66, 42: 55, 41: 68, 40: 63, 39: 82, 38: 75, 37: 85, 36: 90, 35: 94, 34: 88, 33: 104, 32: 133, 31: 107, 30: 140, 29: 150, 28: 148, 27: 155, 26: 162, 25: 200, 24: 219, 23: 241, 22: 273, 21: 276, 20: 325, 19: 352, 18: 451, 17: 527, 16: 594, 15: 693, 14: 777, 13: 940, 12: 1197, 11: 1443, 10: 1756, 9: 2394, 8: 2962, 7: 4232, 6: 6042, 5: 9564, 4: 17201, 3: 45501, 2: 312046, 1: 8674547}\n",
      "top 5 duplicate level reads: [633, 573, 568, 492, 476]\n"
     ]
    }
   ],
   "source": [
    "print('Looking for read duplicates')\n",
    "dico = utils.find_duplicates(reads[:10_000_000], one_hot=True, batch_size=5_000_000, print_freq=1_000_000)\n",
    "values = list(dico.values())\n",
    "print('Looking for duplicate level duplicates')\n",
    "dico2 = utils.find_duplicates(values)\n",
    "dico2 = dict(sorted(dico2.items(), reverse=True))\n",
    "print('{duplicate_level: nb_unique_reads}')\n",
    "print(dico2)\n",
    "print('top 5 duplicate level reads:', list(dico2.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_reads = [k for k, v in dico.items() if v == 1896]\n",
    "print(selected_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(dico.keys())\n",
    "y = list(dico.values())\n",
    "plt.plot(x, y)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('occurences')\n",
    "plt.xlabel('duplicate level')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('shared_folder/Bombyx/dataset3.npz') as dataset:\n",
    "    y_train = dataset['y_train']\n",
    "    y_valid = dataset['y_valid']\n",
    "    y_test = dataset['y_test']\n",
    "print('total number of reads:',len(y_train) + len(y_valid) + len(y_test))\n",
    "print('train:', len(y_train), 'valid:', len(y_valid), 'test:', len(y_test))\n",
    "print('pos_train:', len(y_train[y_train == 1]), 'pos_valid', len(y_valid[y_valid == 1]), 'pos_test', len(y_test[y_test == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot test distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'CENPT'\n",
    "dataset_name = 'dataset2'\n",
    "model_name = 'model_inception2.0'\n",
    "new_labels = 'dataset_rel55'\n",
    "relabeled = False\n",
    "single_pred_array = True\n",
    "if single_pred_array:\n",
    "    if relabeled:\n",
    "        with np.load(f'{data}/results/{model_name}/distrib_{model_name}_test_rel.npz') as f:\n",
    "            preds = f['pred']\n",
    "        with np.load(f'shared_folder/{data}/{new_labels}.npz') as f:\n",
    "            y_test = f['y_test']\n",
    "    else:\n",
    "        with np.load(f'{data}/results/{model_name}/distrib_{model_name}_test.npz') as f:\n",
    "            preds = f['pred']\n",
    "        with np.load(f'shared_folder/{data}/{dataset_name}.npz') as f:\n",
    "            y_test = f['y_test']\n",
    "    predIP = preds[y_test == 1]\n",
    "    predControl = preds[y_test == 0]\n",
    "else:\n",
    "    with np.load(f'{data}/results/distrib_{model_name}_test.npz') as f:\n",
    "            predIP = f['predIP']\n",
    "            predControl = f['predControl']\n",
    "\n",
    "accuracy =  (len(predIP[predIP > 0.5]) + len(predControl[predControl < 0.5])) / (len(predIP) + len(predControl))\n",
    "bins = np.histogram(np.concatenate((predIP, predControl)), bins=100)[1]\n",
    "\n",
    "widths = [5, 1]\n",
    "heights = [1.2] + [2.5]*2\n",
    "fig = plt.figure(figsize=(np.sum(widths), np.sum(heights)))\n",
    "# fig.suptitle(f\"Distribution of predictions of {model_name} on {data} for IP and Control reads\")\n",
    "gs = gridspec.GridSpec(ncols=len(widths), nrows=len(heights), width_ratios=widths, height_ratios=heights)\n",
    "\n",
    "ax0_top = plt.subplot(gs[0, 0])\n",
    "label_col0 = 'predictions'\n",
    "ax0_top.annotate(label_col0, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax0_top.axis('off')\n",
    "\n",
    "ax1_top = plt.subplot(gs[0, 1])\n",
    "label_col1 = 'scale'\n",
    "ax1_top.annotate(label_col1, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax1_top.axis('off')\n",
    "\n",
    "for nrow in range(1,3):\n",
    "\n",
    "    ax0 = plt.subplot(gs[nrow, 0])\n",
    "    ax0.hist(predIP, bins, label='IP', alpha=0.5)\n",
    "    ax0.hist(predControl, bins, label='Control', alpha=0.5)\n",
    "    ax0.axvline(x=0.5, color='black', label='t0.5')\n",
    "    thres = 0.72\n",
    "    ax0.axvline(x=thres, color='red', label=f't{thres}')\n",
    "    \n",
    "    ax1 = plt.subplot(gs[nrow, 1])\n",
    "    if nrow == 1:\n",
    "        label = 'linear'\n",
    "        ax1.annotate(f'accuracy: {round(accuracy, 4)}', (0, 0.4), xycoords='axes fraction', va='center', fontsize=13)\n",
    "    elif nrow == 2:\n",
    "        label = 'log'\n",
    "        ax0.set_yscale('log')\n",
    "    ax1.annotate(label, (0.1, 0.6), xycoords='axes fraction', va='center', fontsize=18)\n",
    "    ax1.axis('off')\n",
    "ax0.set_xlabel(\"predictions\", fontsize=18)\n",
    "ax0.set_ylabel(\"read count\", fontsize=18)\n",
    "handles, labels = ax0.get_legend_handles_labels()\n",
    "ax0_top.legend(handles, labels, loc='right')\n",
    "\n",
    "if not os.path.isdir(f'{data}/results/{model_name}'):\n",
    "    os.mkdir(f'{data}/results/{model_name}/')\n",
    "plt.savefig(f'{data}/results/{model_name}/distrib_{model_name}_test.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'shared_folder/{data}/{dataset_name}.npz') as f:\n",
    "    x_test = f['x_test']\n",
    "# thres = 0.55\n",
    "selected_reads = x_test[preds.ravel() > thres, :, :]\n",
    "print(f'nb of selected reads with thres {thres}:', len(selected_reads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write into fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3m per 10_000_000 seqs of 101 bp\n",
    "utils.write_fasta(utils.one_hot_to_seq(selected_reads), f'{data}/results/{model_name}/seqs_{data}_over_{thres}_with_{model_name}.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(f'{data}/results/{model_name}'):\n",
    "    os.mkdir(f'{data}/results/{model_name}/')\n",
    "logs = pd.read_csv(f'{data}/Trainedmodels/{model_name}/epoch_data.csv')\n",
    "fig = plt.figure(figsize=(5, 6))\n",
    "fig.suptitle(f'Training logs for {model_name} on {data}')\n",
    "ax0 = plt.subplot(211)\n",
    "ax0.plot(logs['val_accuracy'], label='valid')\n",
    "ax0.plot(logs['accuracy'], label='train')\n",
    "ax0.set_ylabel('accuracy')\n",
    "ax0.legend()\n",
    "\n",
    "ax1 = plt.subplot(212)\n",
    "ax1.plot(logs['val_loss'], label='valid')\n",
    "ax1.plot(logs['loss'], label='train')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "plt.legend()\n",
    "plt.savefig(f'{data}/results/{model_name}/{model_name}_train_log.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_accuracy = np.size(predIP[predIP > 0.5]) / np.size(predIP)\n",
    "Control_accuracy = (np.size(predControl[predControl < 0.5])\n",
    "                    / np.size(predControl))\n",
    "print('accuracy: ', (IP_accuracy + Control_accuracy) / 2)\n",
    "print('IP accuracy: ', IP_accuracy)\n",
    "print('Control accuracy: ', Control_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot test distribution during epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'H3K9me3'\n",
    "dataset_name = 'dataset30M'\n",
    "model_name = 'model_inception2'\n",
    "new_labels = 'dataset_rel55'\n",
    "relabeled = False\n",
    "preds = np.load(f'{data}/Trainedmodels/{model_name}/preds_tr.npy')\n",
    "# Watching for nan predictions\n",
    "# for epoch in range(preds.shape[0]):\n",
    "#     nans = np.count_nonzero(np.isnan(preds[epoch]))\n",
    "#     print(f'Epoch {epoch}: {nans} nan predictions')\n",
    "if relabeled:\n",
    "    with np.load(f'{data}/{new_labels}.npz') as f:\n",
    "        y_train = f['y_train']\n",
    "else:\n",
    "    with np.load(f'shared_folder/{data}/{dataset_name}.npz') as f:\n",
    "        y_train = f['y_train']\n",
    "\n",
    "# epochs = range(0, preds.shape[0], 5)\n",
    "epochs = [0, 1, 2, 3, 4, 9, 14, 19]\n",
    "widths = [5, 1]\n",
    "heights = [1.2] + [2]*len(epochs)\n",
    "fig = plt.figure(figsize=(np.sum(widths), np.sum(heights)))\n",
    "# fig.suptitle(f\"Distribution of predictions of {model_name} during epochs on {data} for IP and Control reads\")\n",
    "gs = gridspec.GridSpec(ncols=len(widths), nrows=len(heights), width_ratios=widths, height_ratios=heights)\n",
    "\n",
    "ax0_top = plt.subplot(gs[0, 0])\n",
    "label_col0 = 'prediction'\n",
    "ax0_top.annotate(label_col0, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax0_top.axis('off')\n",
    "\n",
    "ax1_top = plt.subplot(gs[0, 1])\n",
    "label_col1 = 'epoch'\n",
    "ax1_top.annotate(label_col1, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax1_top.axis('off')\n",
    "\n",
    "nrow = 1\n",
    "for epoch in epochs:\n",
    "    preds_epoch = preds[epoch]\n",
    "    predIP = preds_epoch[y_train == 1]\n",
    "    predControl = preds_epoch[y_train == 0]\n",
    "    bins = np.histogram(preds_epoch, bins=100, range=(0,1))[1]\n",
    "\n",
    "    ax0 = plt.subplot(gs[nrow, 0])\n",
    "    ax0.hist(predIP, bins, label='IP', alpha=0.5)\n",
    "    ax0.hist(predControl, bins, label='Control', alpha=0.5)\n",
    "    ax0.axvline(x=0.5, color='black', label='base thres')\n",
    "\n",
    "    ax1 = plt.subplot(gs[nrow, 1])\n",
    "    label = f'{epoch+1}'\n",
    "    ax1.annotate(label, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    nrow += 1\n",
    "\n",
    "handles, labels = ax0.get_legend_handles_labels()\n",
    "ax0_top.legend(handles, labels, loc='right')\n",
    "ax0.set_xlabel(\"predictions\")\n",
    "ax0.set_ylabel(\"read count\")\n",
    "if not os.path.isdir(f'{data}/results/{model_name}'):\n",
    "    os.mkdir(f'{data}/results/{model_name}/')\n",
    "plt.savefig(f'{data}/results/{model_name}/distrib_{model_name}_train_epochs.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold3(pred_pos, pred_neg, choice='first', tolerance=0.9):\n",
    "    \"\"\"\n",
    "    Find the threshold minimizing the misclassification error while satisfying\n",
    "    a condition on the top cumulative positive rate.\n",
    "\n",
    "    Misclassification error for threshold t is defined as #{pos<t} + #{neg>t}.\n",
    "    Top cumulative positive rate (Tcpr) for threshold t is defined as\n",
    "    #{pos>t} / (#{pos>t} + #{neg>t})\n",
    "\n",
    "    If the threshold is reached between 2 values, return the mean of these two\n",
    "    values.\n",
    "    If the threshold is reached on edges, return median of all values\n",
    "    The condition for the threshold is Tcpr > tolerance, which defaults to 0.9\n",
    "\n",
    "    Parameters:\n",
    "    pred_pos (array_like): list of predictions on positive samples\n",
    "    pred_neg (array_like): list of predictions on negative samples\n",
    "    choice (str, optional):\n",
    "        'first' for first occurence, 'last' for last occurence,\n",
    "        defaults to 'first'\n",
    "    tolerance (float, optional): tolerance rate for the Tcpr\n",
    "\n",
    "    Returns:\n",
    "    threshold (float): value of the threshold\n",
    "\n",
    "    Implementation:\n",
    "    Sort separately positive and negative examples predictions, then sort them\n",
    "    together by searching incrementally through both lists. While doing so,\n",
    "    update the misclassification error and top cumulative positive rate. Store\n",
    "    the minimal error reached so far that satisfies the Tcpr condition, as\n",
    "    well as the index of the argmin value. The argmin value is the one that\n",
    "    should be just below the threshold\n",
    "    \"\"\"\n",
    "    pred_neg = np.sort(pred_neg.flatten())\n",
    "    pred_pos = np.sort(pred_pos.flatten())\n",
    "    i = 0\n",
    "    j = 0\n",
    "    # Store tuple indices, first value is 0 for pred_neg or 1 for pred_pos\n",
    "    # and second value is the index in the corresponding array\n",
    "    indices = []\n",
    "    # At the start, all neg values are misclassified and all pos values are\n",
    "    # well classified\n",
    "    error = len(pred_neg)\n",
    "    min_error = error\n",
    "    min_error_index = -1  # index in in the \"indices\" list\n",
    "    # Store number of positives examples and total number of examples above\n",
    "    # current position\n",
    "    top_cur_pos_len = len(pred_pos)\n",
    "    top_cur_tot_len = len(pred_neg) + len(pred_pos)\n",
    "\n",
    "    while (i < len(pred_neg)) or (j < len(pred_pos)):\n",
    "        if i == len(pred_neg):  # End of pred_neg is reached\n",
    "            indices.append((1, j))  # Fetch next point from pred_pos\n",
    "            j += 1\n",
    "            error += 1\n",
    "            top_cur_pos_len -= 1\n",
    "        elif j == len(pred_pos):  # End of pred_pos is reached\n",
    "            indices.append((0, i))  # Fetch next point from pred_neg\n",
    "            i += 1\n",
    "            error -= 1\n",
    "        elif pred_neg[i] < pred_pos[j]:  # Smallest current value\n",
    "            indices.append((0, i))\n",
    "            i += 1\n",
    "            error -= 1\n",
    "        else:\n",
    "            indices.append((1, j))\n",
    "            j += 1\n",
    "            error += 1\n",
    "            top_cur_pos_len -= 1\n",
    "        top_cur_tot_len -= 1\n",
    "        # compute top cumulative positive rate\n",
    "        if top_cur_tot_len != 0:\n",
    "            top_cumul_pos_rate = top_cur_pos_len / top_cur_tot_len\n",
    "        else:\n",
    "            top_cumul_pos_rate = 1\n",
    "        # If error is the lowest so far, update minimal error\n",
    "        if error < min_error and top_cumul_pos_rate > tolerance:\n",
    "            min_error = error\n",
    "            min_error_index = i + j - 1\n",
    "        elif (choice == 'last'\n",
    "              and error == min_error\n",
    "              and top_cumul_pos_rate > tolerance):\n",
    "            min_error_index = i + j - 1\n",
    "\n",
    "    if (min_error_index == -1\n",
    "            or min_error_index == len(pred_neg)+len(pred_pos)-1):\n",
    "        # If minimum is reached on the edges, return median as default\n",
    "        median = np.median(np.concatenate((pred_pos, pred_neg)))\n",
    "        print('median', median)  # ######################################\n",
    "        return median\n",
    "    # Otherwise, return the mean of the two points on either side of the\n",
    "    # threshold\n",
    "    list_id_below, value_index_below = indices[min_error_index]\n",
    "    list_id_above, value_index_above = indices[min_error_index+1]\n",
    "    if list_id_below == 0:  # Take from pred_neg\n",
    "        value_below = pred_neg[value_index_below]\n",
    "    else:  # Take from pred_pos\n",
    "        value_below = pred_pos[value_index_below]\n",
    "    if list_id_above == 0:\n",
    "        value_above = pred_neg[value_index_above]\n",
    "    else:\n",
    "        value_above = pred_pos[value_index_above]\n",
    "    threshold = (value_below + value_above) / 2\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def find_threshold4(pred_pos, pred_neg, nb_bins=200):\n",
    "    \"\"\"\n",
    "    Find the intersection of the normalized cumulative distribution of pos and\n",
    "    neg predictions.\n",
    "\n",
    "    Normalization is performed wrt the area under the cumulative distribution,\n",
    "    which is not standard normalization\n",
    "\n",
    "    The threshold is computed through bins, resulting in approximation\n",
    "    Last intersection is computed\n",
    "\n",
    "    Parameters:\n",
    "    pred_pos (array_like): list of predictions on positive samples\n",
    "    pred_neg (array_like): list of predictions on negative samples\n",
    "    nb_bins (int, optional): number of bins to divide the range of predictions\n",
    "        into, default to 200\n",
    "\n",
    "    Returns:\n",
    "    threshold (float): value of the threshold\n",
    "    \"\"\"\n",
    "    bins = np.linspace(0, 1, nb_bins)\n",
    "    pos_bins = np.digitize(pred_pos, bins).flatten()\n",
    "    pos_count = np.bincount(pos_bins, minlength=len(bins)+1)[1:]\n",
    "    cumul_pos_count = np.cumsum(pos_count)\n",
    "    total_pos_count = np.sum(cumul_pos_count)\n",
    "    cumul_pos_count_norm = cumul_pos_count / total_pos_count\n",
    "    cumul_pos_count_norm[np.isnan(cumul_pos_count_norm)] = 1\n",
    "\n",
    "    neg_bins = np.digitize(pred_neg, bins).flatten()\n",
    "    neg_count = np.bincount(neg_bins, minlength=len(bins)+1)[1:]\n",
    "    cumul_neg_count = np.cumsum(neg_count)\n",
    "    total_neg_count = np.sum(cumul_neg_count)\n",
    "    cumul_neg_count_norm = cumul_neg_count / total_neg_count\n",
    "    cumul_neg_count_norm[np.isnan(cumul_neg_count_norm)] = 1\n",
    "\n",
    "    reverse_list = (cumul_pos_count_norm < cumul_neg_count_norm)[::-1]\n",
    "    index = len(reverse_list) - np.argmax(reverse_list) - 1\n",
    "    threshold = bins[index]\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def PU_threshold(pred_neg_valid):\n",
    "    \"\"\"\n",
    "    Using PU learning insight to choose threshold.\n",
    "    \"\"\"\n",
    "    neg_recall = 1 - np.mean(pred_neg_valid)\n",
    "    threshold = 1 - neg_recall/2\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def threshold6(preds, pred_neg_valid):\n",
    "    \"\"\"\n",
    "    Use PU learning to estimate noise rate, then chose threshold.\n",
    "\n",
    "    Noise rate is estimated as in PU_threshold, the threshold is set to keep\n",
    "    as many examples above it as there are true positives\n",
    "    \"\"\"\n",
    "    # Estimate noise rate\n",
    "    neg_recall = 1 - np.mean(pred_neg_valid)\n",
    "    tp_estim = 2 - 1 / neg_recall\n",
    "    # Sort predictions\n",
    "    preds = np.sort(preds.flatten())\n",
    "    # Compute index of first value above threshold\n",
    "    sep = int(len(preds) * (2 - tp_estim) / 2)\n",
    "    if sep >= len(preds) or sep <= len(preds)/2:\n",
    "        # If index is out of bounds, return median as default\n",
    "        median = np.median(preds)\n",
    "        print('median', median)  # ######################################\n",
    "        return median\n",
    "    else:\n",
    "        # Compute threshold as mean of the values on either side\n",
    "        thres = (preds[sep-1] + preds[sep]) / 2\n",
    "        return thres\n",
    "\n",
    "\n",
    "def threshold7(preds, y, pred_neg_valid):\n",
    "    \"\"\"\n",
    "    Using PU learning to estimate noise rate, then estimate best accuracy.\n",
    "\n",
    "    Noise rate is estimated as in PU_threshold, accuracy is computed\n",
    "    considering the top postive predictions as true positives and others as\n",
    "    false positives. The true positives are weighted to balance classes.\n",
    "\n",
    "    Implementation:\n",
    "    Compute number of true positives by rounding from true positive rate and\n",
    "    number of positives. Compute weights according to the number of true\n",
    "    positives.\n",
    "    Sort labels in order of increasing predictions. Compute number of pos and\n",
    "    neg examples under a given position in the sorted array with cumulative\n",
    "    sums over the labels. Compute the error by identifying wrongly predicted\n",
    "    examples. Wrongly predicted examples are:\n",
    "    - neg examples over;\n",
    "    - if there are more pos examples over then there are true positives, the\n",
    "    surplus of positives are wrong;\n",
    "    - otherwise the remaining true positives under are wrong with weighted\n",
    "    contribution to the error\n",
    "    The argmin value should be the one just below the threshold. Since the\n",
    "    predictions weren't sorted, fetch the predictions corresponding to given\n",
    "    index using the stored order from argsort.\n",
    "    \"\"\"\n",
    "    # Estimate noise rate\n",
    "    neg_recall = 1 - np.mean(pred_neg_valid)\n",
    "    tp_estim = 2 - 1 / neg_recall\n",
    "    # Number of positives and true positives\n",
    "    pos_len = len(y[y == 1])\n",
    "    n_tp_estim = round(pos_len * tp_estim)\n",
    "    # Weight of true positives to balance classes\n",
    "    weight_tp = (len(y) - n_tp_estim) / n_tp_estim\n",
    "    # Sort arrays\n",
    "    preds, y = preds.flatten(), y.flatten()\n",
    "    order = np.argsort(preds)\n",
    "    y = y[order]\n",
    "    # Compute the number of pos and neg examples on either side of thres for\n",
    "    # every position of thres\n",
    "    neg_under = np.cumsum(1 - y)\n",
    "    neg_over = len(y) - pos_len - neg_under\n",
    "    pos_under = np.cumsum(y)\n",
    "    pos_over = pos_len - pos_under\n",
    "    # Estimate error at each position, and extract minimum\n",
    "    error = np.where(n_tp_estim <= pos_over,\n",
    "                     neg_over + pos_over - n_tp_estim,\n",
    "                     neg_over + weight_tp*(n_tp_estim - pos_over))\n",
    "    min_error_index = np.argmin(error)\n",
    "    if min_error_index == len(y) - 1:\n",
    "        # If minimum is reached at the end, return median as default\n",
    "        median = np.median(preds)\n",
    "        print('median', median)  # ######################################\n",
    "        return median\n",
    "    else:\n",
    "        # compute threshold as mean of the values on either side\n",
    "        thres = (preds[order[min_error_index]]\n",
    "                 + preds[order[min_error_index+1]]) / 2\n",
    "        return thres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('H3K27ac/results/distrib_model_inception_train.npz') as f:\n",
    "    predIP_tr = f['predIP_train']\n",
    "    predControl_tr = f['predControl_train']\n",
    "    predControl_v = f['predControl_valid']\n",
    "with np.load('H3K27ac/dataset.npz') as f:\n",
    "    y_train = f['y_train']\n",
    "preds = np.concatenate((predIP_tr, predControl_tr))\n",
    "y = np.concatenate((np.ones(len(predIP_tr)), np.zeros(len(predControl_tr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "t6 = threshold6(preds, predControl_v)\n",
    "print('t6:', time.time() - t0)\n",
    "t0 = time.time()\n",
    "t7 = threshold7(preds, y, predControl_v)\n",
    "print('t7:', time.time() - t0)\n",
    "t0 = time.time()\n",
    "t3 = find_threshold3(predIP_tr, predControl_tr)\n",
    "print('t3:', time.time() - t0)\n",
    "t0 = time.time()\n",
    "t_PU = PU_threshold(predControl_v)\n",
    "print('t_PU:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_estim = 2 - 0.5 / (1 - t_PU) # estimation of the true positive rate with tPU\n",
    "print(tp_estim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.histogram(np.concatenate((predIP_tr, predControl_tr)), bins=100)[1]\n",
    "plt.figure()\n",
    "plt.hist(predIP_tr,bins,label='IP',alpha=0.5)\n",
    "plt.hist(predControl_tr,bins,label='Control',alpha=0.5)\n",
    "plt.axvline(t_PU, label='tPU', color='black')\n",
    "plt.axvline(t3, label='t3', color='blue')\n",
    "plt.axvline(t6, label='t6', color='green')\n",
    "plt.axvline(t7, label='t7', color='red')\n",
    "plt.title(\"Distribution of the probability outputs for IP and Control reads\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"bin count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_bins = 200\n",
    "bins = np.linspace(0,1,nb_bins)\n",
    "IP_bins = np.digitize(predIP, bins).flatten()\n",
    "IP_count = np.bincount(IP_bins, minlength=len(bins)+1)[1:]\n",
    "Control_bins = np.digitize(predControl, bins).flatten()\n",
    "Control_count = np.bincount(Control_bins, minlength=len(bins)+1)[1:]\n",
    "total_count = IP_count + Control_count\n",
    "IP_rate = IP_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cumul_IP_count = np.flip(np.cumsum(np.flip(IP_count)))\n",
    "top_cumul_total_count = np.flip(np.cumsum(np.flip(total_count)))\n",
    "top_cumul_IP_rate = top_cumul_IP_count / top_cumul_total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(predIP, bins=200, label='IP', weights=20*np.ones(len(predIP)) / len(predIP), alpha=0.5)\n",
    "plt.hist(predControl, bins=200, label='Control', weights=20*np.ones(len(predControl)) / len(predControl), alpha=0.5)\n",
    "plt.plot(bins, IP_rate, label='IP rate')\n",
    "plt.plot(bins, top_cumul_IP_rate, label='cumulative IP rate from top', color='black')\n",
    "plt.axhline(y=0.9,label='0.9 rate limit', color='red')\n",
    "plt.title(\"rate of IP reads per prediction value\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"rate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot GC_content distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='NONO'\n",
    "dataset_name='dataset2'\n",
    "with np.load(f'shared_folder/{data}/{dataset_name}.npz') as f:\n",
    "    x_test = f['x_test']\n",
    "    y_test = f['y_test']\n",
    "gc_content = utils.GC_content(x_test)\n",
    "\n",
    "bins = np.histogram(gc_content, bins=100, range=(0, 1))[1]\n",
    "accuracy, thres = utils.classify_1D(gc_content, y_test, bins=100)\n",
    "print('accuracy :', accuracy)\n",
    "\n",
    "widths = [5, 1]\n",
    "heights = [1.2] + [2.5]*2\n",
    "fig = plt.figure(figsize=(np.sum(widths), np.sum(heights)))\n",
    "# fig.suptitle(f\"Distribution of predictions of {model_name} on {data} for IP and Control reads\")\n",
    "gs = gridspec.GridSpec(ncols=len(widths), nrows=len(heights), width_ratios=widths, height_ratios=heights)\n",
    "\n",
    "ax0_top = plt.subplot(gs[0, 0])\n",
    "label_col0 = 'GC content'\n",
    "ax0_top.annotate(label_col0, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax0_top.axis('off')\n",
    "\n",
    "ax1_top = plt.subplot(gs[0, 1])\n",
    "label_col1 = 'scale'\n",
    "ax1_top.annotate(label_col1, (0.1, 0.5), xycoords='axes fraction', va='center', fontsize=18)\n",
    "ax1_top.axis('off')\n",
    "\n",
    "for nrow in range(1,3):\n",
    "\n",
    "    ax0 = plt.subplot(gs[nrow, 0])\n",
    "    ax0.hist(gc_content[y_test == 1], bins, label='IP', alpha=0.5)\n",
    "    ax0.hist(gc_content[y_test == 0], bins, label='Control', alpha=0.5)\n",
    "    ax0.axvline(x=thres, color='black', label='thres')\n",
    "    \n",
    "    ax1 = plt.subplot(gs[nrow, 1])\n",
    "    if nrow == 1:\n",
    "        scale_label = 'linear'\n",
    "        ax1.annotate(f'accuracy: {round(accuracy, 4)}', (0, 0.4), xycoords='axes fraction', va='center', fontsize=13)\n",
    "    elif nrow == 2:\n",
    "        scale_label = 'log'\n",
    "        ax0.set_yscale('log')\n",
    "    ax1.annotate(scale_label, (0.1, 0.6), xycoords='axes fraction', va='center', fontsize=18)\n",
    "    ax1.axis('off')\n",
    "ax0.set_xlabel(\"GC content\", fontsize=18)\n",
    "ax0.set_ylabel(\"read count\", fontsize=18)\n",
    "handles, labels = ax0.get_legend_handles_labels()\n",
    "ax0_top.legend(handles, labels, loc='right')\n",
    "\n",
    "fig.suptitle(f'Distribution of GC content in {data} reads', fontsize=18)\n",
    "plt.savefig(f'{data}/results/gc_content_classification_{dataset_name}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00e7424330818c90ac25a9eb217b83ab19236ec3c884b7e4b78b97ecfb399601"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf2.5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
